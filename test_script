#!/usr/bin/env python3
"""
Test script for SynthesisTalk LLM Integration
Run this to verify your LLM setup is working correctly
"""

import os
import sys
import json
from datetime import datetime

# Add current directory to path so we can import our modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_environment():
    """Test environment variables and configuration"""
    print("üîß Testing Environment Configuration...")
    
    required_vars = ['MODEL_SERVER', 'NGU_API_KEY', 'NGU_BASE_URL', 'NGU_MODEL']
    missing_vars = []
    
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            missing_vars.append(var)
        else:
            # Don't print API keys in full
            if 'API_KEY' in var:
                display_value = value[:8] + "..." if len(value) > 8 else "***"
            else:
                display_value = value
            print(f"  ‚úÖ {var}: {display_value}")
    
    if missing_vars:
        print(f"  ‚ùå Missing environment variables: {', '.join(missing_vars)}")
        return False
    
    return True

def test_basic_llm():
    """Test basic LLM functionality"""
    print("\nü§ñ Testing Basic LLM Functionality...")
    
    try:
        from llm_client import ask_llm
        
        # Simple test
        response = ask_llm("Hello! Can you introduce yourself?", use_tools=False)
        print(f"  ‚úÖ Basic LLM Response: {response[:100]}...")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Basic LLM test failed: {str(e)}")
        return False

def test_advanced_llm():
    """Test advanced LLM features"""
    print("\nüî¨ Testing Advanced LLM Features...")
    
    try:
        from llm_client import SynthesisTalkLLM
        
        llm = SynthesisTalkLLM()
        
        # Test Chain of Thought reasoning
        print("  Testing Chain of Thought reasoning...")
        response = llm.chat(
            "What are the benefits of using AI in education?",
            reasoning_type="chain_of_thought"
        )
        print(f"    ‚úÖ CoT Response: {response['response'][:100]}...")
        
        # Test ReAct reasoning
        print("  Testing ReAct reasoning...")
        response = llm.chat(
            "How can I research a complex topic effectively?",
            reasoning_type="react"
        )
        print(f"    ‚úÖ ReAct Response: {response['response'][:100]}...")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Advanced LLM test failed: {str(e)}")
        return False

def test_tools():
    """Test tool functionality"""
    print("\nüõ†Ô∏è  Testing Tool Integration...")
    
    try:
        from llm_client import SynthesisTalkLLM
        
        llm = SynthesisTalkLLM()
        
        # Test note-taking tool
        print("  Testing note-taking tool...")
        result = llm.tool_manager.execute_tool("save_note", {
            "title": "Test Note",
            "content": "This is a test note created during integration testing.",
            "tags": ["test", "integration"]
        })
        
        if result.success:
            print(f"    ‚úÖ Note saved successfully: {result.data['id']}")
        else:
            print(f"    ‚ùå Note saving failed: {result.error}")
            return False
        
        # Test web search tool
        print("  Testing web search tool...")
        result = llm.tool_manager.execute_tool("web_search", {
            "query": "artificial intelligence",
            "num_results": 3
        })
        
        if result.success:
            print(f"    ‚úÖ Web search successful: {len(result.data)} results")
        else:
            print(f"    ‚ùå Web search failed: {result.error}")
        
        # Test explanation tool
        print("  Testing explanation tool...")
        result = llm.tool_manager.execute_tool("explain_concept", {
            "concept": "machine learning",
            "level": "intermediate"
        })
        
        if result.success:
            print(f"    ‚úÖ Explanation generated successfully")
        else:
            print(f"    ‚ùå Explanation failed: {result.error}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Tool testing failed: {str(e)}")
        return False

def test_document_analysis():
    """Test document analysis functionality"""
    print("\nüìÑ Testing Document Analysis...")
    
    try:
        from llm_client import SynthesisTalkLLM
        
        # Create a test document
        test_content = """
        Artificial Intelligence in Healthcare
        
        Artificial Intelligence (AI) is revolutionizing healthcare by enabling more accurate diagnoses,
        personalized treatment plans, and improved patient outcomes. Key applications include:
        
        1. Medical Imaging: AI algorithms can analyze X-rays, MRIs, and CT scans with high accuracy.
        2. Drug Discovery: Machine learning accelerates the identification of potential drug compounds.
        3. Predictive Analytics: AI helps predict patient deterioration and treatment responses.
        
        The integration of AI in healthcare promises to enhance efficiency and reduce costs while
        improving the quality of care for patients worldwide.
        """
        
        # Write test document
        test_file = "test_document.txt"
        with open(test_file, 'w') as f:
            f.write(test_content)
        
        llm = SynthesisTalkLLM()
        
        # Test document analysis
        result = llm.analyze_document(test_file, "summary")
        
        if result['success']:
            print(f"    ‚úÖ Document analysis successful")
            print(f"    Summary: {result['content'][:150]}...")
        else:
            print(f"    ‚ùå Document analysis failed: {result['error']}")
            return False
        
        # Clean up
        if os.path.exists(test_file):
            os.remove(test_file)
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Document analysis test failed: {str(e)}")
        return False

def test_conversation_flow():
    """Test multi-turn conversation"""
    print("\nüí¨ Testing Conversation Flow...")
    
    try:
        from llm_client import SynthesisTalkLLM
        
        llm = SynthesisTalkLLM()
        
        # Multi-turn conversation
        conversations = [
            "Hello, I'm researching renewable energy sources.",
            "Can you tell me about solar energy specifically?",
            "What are the main challenges with solar energy adoption?",
            "How does solar energy compare to wind energy?"
        ]
        
        for i, message in enumerate(conversations, 1):
            response = llm.chat(message)
            print(f"    Turn {i}: {response['response'][:80]}...")
        
        # Check conversation history
        history = llm.conversation_manager.messages
        print(f"    ‚úÖ Conversation maintained: {len(history)} messages in history")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Conversation flow test failed: {str(e)}")
        return False

def test_api_compatibility():
    """Test API compatibility"""
    print("\nüåê Testing API Compatibility...")
    
    try:
        # Test if FastAPI components can be imported
        from main import app  # Assuming you save the FastAPI code as main.py
        print("    ‚úÖ FastAPI app imports successfully")
        
        # Test Pydantic models
        from main import ChatRequest, ChatResponse
        
        # Create test request
        test_request = ChatRequest(
            message="Test message",
            use_tools=True,
            reasoning_type="chain_of_thought"
        )
        print("    ‚úÖ Pydantic models work correctly")
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå API compatibility test failed: {str(e)}")
        print("    Note: This is expected if you haven't saved the FastAPI code as main.py yet")
        return False

def run_all_tests():
    """Run all tests and provide summary"""
    print("üöÄ Starting SynthesisTalk LLM Integration Tests")
    print("=" * 60)
    
    tests = [
        ("Environment Configuration", test_environment),
        ("Basic LLM Functionality", test_basic_llm),
        ("Advanced LLM Features", test_advanced_llm),
        ("Tool Integration", test_tools),
        ("Document Analysis", test_document_analysis),
        ("Conversation Flow", test_conversation_flow),
        ("API Compatibility", test_api_compatibility),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"    ‚ùå {test_name} crashed: {str(e)}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä Test Results Summary:")
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All tests passed! Your LLM integration is ready.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Check the errors above and fix the issues.")
    
    return passed == total

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)